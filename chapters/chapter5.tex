\chapter{Statistical Inference Using the
Morse-Smale Complex}

It is natural to think that be could treat the problem
of aproximating the homology of a manifold from a finite 
sample from a stastical point of view. 
That is what we will try to do in this chapter. It will be based on the findings of the paper: \cite{che2017}.

The main idea is to use a decomposition similar
to the one given in \ref{handle}.

As in the previous sections we assume we have a closed d-manifold $M^d\subset \mathbb{R}^n$
and we have a Morse function on it $f:M^d\rightarrow \mathbb{R}$.

And we call the $\omega$-limit of $x\in M$ to:
$\omega^-(x)=\displaystyle\lim_{t\to \infty} \phi^t(x)$ where $\phi^t$ is 
the flow generated by $\nabla f$.

We define the descendings k-manifolds as the inverse image by $\omega^-$ of $Crit_{d-k}(f)$. 
We recall this are the $k$-index critical points of $f$.
In this case, use this Morse function usually is a density function.
Formally:
$$
D_k=
(\omega^-)^{-1}(Crit_{d-k})
$$

Since $Crit_k$ is finite because $M$ is closed we can index it:
$Crit_k=\{c_{k,1},\ldots,c_{k,m_k}\}$ and we can define analogously:
$$
D_{k,j}=
(\omega^-)^{-1}(C_{d-k,j})
$$

Obviously, $D_k=\coprod D_{k,j}$. (By $\coprod$ we mean disjoint union). And similarly
$M=\coprod_i D_i$.

We define the ascending d-manifold in the same way except replacing $\phi^t$ by the flow generated by $-\nabla f$
and we denote them $A_k$.

\begin{remark}
We call $D_k$ and $A_k$
descending and ascending manifolds to mantain coherence with the article \cite{che2017}.
But in the literature it usually used a very similar concept called stable and unstable manifolds (for example
see \cite{che2014}).
And they are denoted $W^s(p,q)$ and $W^u(p,q)$.
They key difference is that stable and unstable manifolds measure the flow between two critical points, not the total
incoming (resp outcoming) flow to a critical point. Needless to say a descending (ascending) manifold is the disjoint 
union of stable (unstable) manifolds.
At least in the compact case.
\end{remark}

\begin{definition}[$d$-cell]
    \label{cell}
    In this chapter we will refer to a $d$-cell to the intersection of a $d$-descending manifold
    and $0$-ascending manifolds.
\end{definition}

Note that $k$-descending manifolds for $k<d$ and $l$-ascending manifolds for $l>0$
have measure $0$. Therefore, $M$ can be decomposed as $M=E_1\cup\cdots E_K\cup N$ where $E_i$ 
are $d$-cells and $N$ is null set.

We refer to this decomposition as the cell decomposition of $M$.
\section{Estimating the Morse function}
Let's take a finite sample from $M$: $\mathcal{X}=\{X_1,\ldots,X_m\}$. 
If $f$ is a density function we take the sample $\mathcal{X}$ following the distribution associated to $f$ (that is 
the one with PDF $p=f/\int_\mathbb{R} f$).

However, in previous chapter we have supposed we only know $f\vert_\mathcal{X}$, so we must construct
$\hat{f}$ an estimation of $f$.

This is usually done by the following estimation:

\begin{definition}[KDE]
\label{kde}
Given $f$ a Morse function of which only $f\vert_\mathcal{X}$ is known, where $\mathcal{X}$ 
is a finite sample on a Manifold $M$.
We call the kernel density estimator or KDE and denote $\hat{f}$ to:
$$
\hat{f}=
\frac{1}{mh^d}
\sum_{i=1}^m
K\left(
\frac{x-X_i}{h}
\right)
$$
\end{definition}

We could choose $K$ and $h$ in different ways. We just require that $K$ is a smooth kernel function, as we
described in \ref{skernel} and $h>0$.
Since $\hat{f}$ is clearly $\mathcal{C}^\infty$
we can consider $\nabla\hat{f}$ and compute $\hat\omega^-$
accordingly.



\section{Mode clustering}
The key point of mode clustering is to use descending $d$-manifolds to partition $M$ into "clusters".

Let $\mathcal{X}$ be a sample as before and let $\{m_1,\ldots,m_l\}$ be the local maxima (or modes) of $\hat{f}$. Then we partition $\mathcal{X}=\coprod\mathcal{X}_i$
where 
$$
\mathcal{X}_i
=
\{
    X_j\in\mathcal{X}\vert \omega^-(X_j)=m_i
\}
$$

\subsection{Analysis of mode clustering}


\section{Mean Shift Algorithm}

For the next sections we need to estimate the cell decomposition of $M$, this can be done
using the mean shift algorithm or the quick-shift algorithm.

\section{Morse-Smale Regression}

Let $(X,Y)$ be a random pair, where $X\in M$ and $Y\in\mathbb{R}$.
Then we define the regression function $m(x)$ as:
$$
m(x)=\mathbb{E}[Y\vert X=x]
$$
This function is not necessarily easy to compute, specially when the dimension $d$ increases.
So we can use simpler function, that estimate $m(x)$. The error will increase but hopefully not too much.
The simplest way to do this is to choose a piecewise linear estimator.

There are two versions of the Morse-Smale regression:

\begin{itemize}

    \item Using $m(x)$ version:

        Let $E_1,\ldots,E_o$ be the $d$-cells that decompose $M$ using $m(x)$ 
        as a Morse function,
then we estimate m as:
$$
\hat{m}(x)=\mu_i+\beta_l^T x, x\in E_i
$$
where we choose $\mu_i$ and $\beta_l^T$ so that they minimise the mean square error:
$$
(\mu_i,\beta_i)
=
\text{argmin}_{\mu,\beta}
\mathbb{E}[
    (
    Y-
    \mu-\beta^X
    )^2
    \vert X\in E_i
]
$$

\item Sample version:
    Let $(X_1,Y_1),\ldots,(X_m,Y_m)$ be a random sample of values, where $X_i$ are sample from $M$
        and $Y_i=\hat f(X_i)$. Then we estimate $m(x)$ using the pilot estimator:
        $$
        \bar{m}(x)=
        \frac{
            \sum Y_iK(\frac{x-X_i}{h})
        }
        {
            \sum K(\frac{x-X_i}{h})
        }
        $$
        Then again we decompose $M$ into $d$-cell using  $\bar m$ as a Morse function:$\hat E_1,\ldots,\hat E_l$
        And we calculate $\hat{m}$ as in the previous point, but changing $E_i$ for $\hat E_i$.
\end{itemize}


\section{Morse-Smale Signatures}

In section \ref{bottle} we say a way to compare two persistence diagrams. In this section we explore a related way to compare
two finite samples from the manifold $M$:
Morse-Smale singatures.
These samples are taken from two different distributions (that is Morse functions).

Suppose we have $X_1,\ldots,X_m$ and $Y_1,\ldots,Y_k$ two samples from $M$.  
They are respectively distributed according to the pdfs $p_X$ and $p_Y$.

We would like to do the study the folowing statistical test:

$$
H_0(x):p_X(x)=p_y(x)
$$

In particuar we are interested when we reject the null hypothesis $H_0$.
To do this, from each pdf we construct the KDE as described in \label{kde},
which we denote $\hat p_X$ and $\hat p_y$ respectively.



Then we consider: $d=\hat p_X - \hat p_Y$.
And we reject $H_0$ when $d(x)>\lambda$ where we choose $\lambda>0$.
In particular, we define:
$$
\Gamma(\lambda)=
\{
    x\in M:
    \vert \hat d \vert > \lambda
\}
$$

So $\Gamma(\lambda)$ is the reject manifold.

\section{Some remarks}
